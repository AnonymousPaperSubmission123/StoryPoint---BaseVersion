{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vega'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8288\\834810430.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconversation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConversationBufferWindowMemory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConversationChain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mvega\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVegaLite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vega'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "# handle GPT API\n",
    "from langchain import LLMChain\n",
    "\n",
    "# formats the prompt history in a particular way\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from vega import VegaLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.renderers.enable('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_state = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/covid_worldwide.csv\")\n",
    "column_names = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_initialisation(TEMPERATURE, MODEL, K, column_names):\n",
    "    # custom query template --> possible to add few shot examples in the future\n",
    "    # add dynamic variables columns and data types to the prompt\n",
    "    template = (\n",
    "        \"\"\"\n",
    "    You are a great assistant at vega-lite visualization creation. No matter what\n",
    "    the user ask, you should always response with a valid vega-lite specification\n",
    "    in JSON.\n",
    "\n",
    "    You should create the vega-lite specification based on user's query.\n",
    "\n",
    "    Besides, Here are some requirements:\n",
    "    1. Do not contain the key called 'data' in vega-lite specification.\n",
    "    2. If the user ask many times, you should generate the specification based on the previous context.\n",
    "    3. You should consider to aggregate the field if it is quantitative and the chart has a mark type of react, bar, line, area or arc.\n",
    "    4. The available fields in the dataset are:\n",
    "    %s\n",
    "    5. Always respond with exactly one vega-lite specfication. Not more, not less.\n",
    "    6. If you use a color attribute, it must be inside the encoding block attribute of the specification.\n",
    "    7. When the user tells you to give him a sample graph, then you give him a vega-lite specification that you think,\n",
    "    will look good.\n",
    "    8. remember to only respond with vega-lite specifications without additional explanations\n",
    "\n",
    "    Current conversation:\n",
    "    {history}\n",
    "    Human: {input}\n",
    "    AI Assistant:\"\"\"\n",
    "        % column_names\n",
    "    )\n",
    "\n",
    "    PROMPT = PromptTemplate(\n",
    "        input_variables=[\"history\", \"input\"], template=template\n",
    "    )\n",
    "\n",
    "    # Create an OpenAI instance\n",
    "    llm = OpenAI(\n",
    "        temperature=TEMPERATURE,\n",
    "        openai_api_key=\"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\",\n",
    "        model_name=MODEL,\n",
    "        verbose=False,\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "    # Create a ConversationEntityMemory object if not already created\n",
    "    if \"entity_memory\" not in session_state:\n",
    "        session_state[\"entity_memory\"] = ConversationBufferWindowMemory(k=K)\n",
    "\n",
    "    # Create the ConversationChain object with the specified configuration\n",
    "    Conversation = ConversationChain(\n",
    "        llm=llm,\n",
    "        prompt=PROMPT,\n",
    "        memory=session_state[\"entity_memory\"],\n",
    "    )\n",
    "\n",
    "    return Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(session_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conversation = model_initialisation(\n",
    "MODEL=\"gpt-3.5-turbo\",\n",
    "TEMPERATURE=1,\n",
    "K=3,\n",
    "column_names=column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\"fig_gpt_1_description\":{\n",
    "\"mark\":\"arc\"\n",
    "\"encoding\":{\n",
    "\"theta\":{\n",
    "\"field\":\"Total_Deaths\"\n",
    "\"type\":\"quantitative\"\n",
    "\"aggregate\":\"average\"\n",
    "}\n",
    "\"color\":{\n",
    "\"field\":\"Country\"\n",
    "\"type\":\"nominal\"\n",
    "}\n",
    "\"order\":{\n",
    "\"field\":\"Total_Deaths\"\n",
    "\"type\":\"quantitative\"\n",
    "\"aggregate\":\"average\"\n",
    "}\n",
    "}\n",
    "\"projection\":{\n",
    "\"type\":\"identity\"\n",
    "}\n",
    "\"data\":{\n",
    "\"values\":[\n",
    "0:{\n",
    "\"Total_Deaths\":21689\n",
    "\"Country\":\"Austria\"\n",
    "}\n",
    "1:{\n",
    "\"Total_Deaths\":68399\n",
    "\"Country\":\"Japan\"\n",
    "}\n",
    "2:{\n",
    "\"Total_Deaths\":697074\n",
    "\"Country\":\"Brazil\"\n",
    "}\n",
    "3:{\n",
    "\"Total_Deaths\":1132935\n",
    "\"Country\":\"USA\"\n",
    "}\n",
    "4:{\n",
    "\"Total_Deaths\":74\n",
    "\"Country\":\"DPRK\"\n",
    "}\n",
    "5:{\n",
    "\"Total_Deaths\":16356\n",
    "\"Country\":\"Taiwan\"\n",
    "}\n",
    "6:{\n",
    "\"Total_Deaths\":111020\n",
    "\"Country\":\"Ukraine\"\n",
    "}\n",
    "7:{\n",
    "\"Total_Deaths\":35630\n",
    "\"Country\":\"Greece\"\n",
    "}\n",
    "8:{\n",
    "\"Total_Deaths\":165711\n",
    "\"Country\":\"Germany\"\n",
    "}\n",
    "9:{\n",
    "\"Total_Deaths\":142486\n",
    "\"Country\":\"Colombia\"\n",
    "}\n",
    "]\n",
    "}\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"mark\": \"arc\",\\n  \"encoding\": {\\n    \"theta\": {\\n      \"field\": \"Total_Deaths\",\\n      \"type\": \"quantitative\",\\n      \"aggregate\": \"average\"\\n    },\\n    \"color\": {\\n      \"field\": \"Country\",\\n      \"type\": \"nominal\"\\n    },\\n    \"order\": {\\n      \"field\": \"Total_Deaths\",\\n      \"type\": \"quantitative\",\\n      \"aggregate\": \"average\"\\n    }\\n  },\\n  \"projection\": {\\n    \"type\": \"identity\"\\n  }\\n}'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conversation.run(input=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "def get_low_level_values(nested_dict):\n",
    "    values = []\n",
    "    for value in nested_dict.values():\n",
    "        if isinstance(value, dict):\n",
    "            values.extend(get_low_level_values(value))\n",
    "        else:\n",
    "            values.append(value)\n",
    "    return values\n",
    "\n",
    "# Example nested dictionary\n",
    "nested_dict = {\n",
    "    'a': 1,\n",
    "    'b': {\n",
    "        'c': 2,\n",
    "        'd': {\n",
    "            'e': 3,\n",
    "            'f': 4\n",
    "        }\n",
    "    },\n",
    "    'g': 5\n",
    "}\n",
    "\n",
    "low_level_values = get_low_level_values(nested_dict)\n",
    "print(low_level_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
